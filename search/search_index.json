{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"introduction/","title":"This should be the introduction to the project","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur. Donec ut libero sed arcu vehicula ultricies a non tortor. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut gravida lorem. Ut turpis felis, pulvinar a semper sed, adipiscing id dolor. Pellentesque auctor nisi id magna consequat sagittis. Curabitur dapibus enim sit amet elit pharetra tincidunt feugiat nisl imperdiet. Ut convallis libero in urna ultrices accumsan. Donec sed odio eros. Donec viverra mi quis quam pulvinar at malesuada arcu rhoncus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In rutrum accumsan ultricies. Mauris vitae nisi at sem facilisis semper ac in est.</p>"},{"location":"supervised-learning/","title":"Introduction to Supervised Learning","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur. Donec ut libero sed arcu vehicula ultricies a non tortor. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut gravida lorem. Ut turpis felis, pulvinar a semper sed, adipiscing id dolor. Pellentesque auctor nisi id magna consequat sagittis. Curabitur dapibus enim sit amet elit pharetra tincidunt feugiat nisl imperdiet. Ut convallis libero in urna ultrices accumsan. Donec sed odio eros. Donec viverra mi quis quam pulvinar at malesuada arcu rhoncus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In rutrum accumsan ultricies. Mauris vitae nisi at sem facilisis semper ac in est.</p>"},{"location":"supervised-learning/classification/","title":"Introduction to Classification Algorithms in Machine Learning","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur. Donec ut libero sed arcu vehicula ultricies a non tortor. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut gravida lorem. Ut turpis felis, pulvinar a semper sed, adipiscing id dolor. Pellentesque auctor nisi id magna consequat sagittis. Curabitur dapibus enim sit amet elit pharetra tincidunt feugiat nisl imperdiet. Ut convallis libero in urna ultrices accumsan. Donec sed odio eros. Donec viverra mi quis quam pulvinar at malesuada arcu rhoncus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In rutrum accumsan ultricies. Mauris vitae nisi at sem facilisis semper ac in est.</p>"},{"location":"supervised-learning/classification/k-nearest-neighbors/","title":"K-Nearest Neighbors (KNN)","text":"In\u00a0[1]: Copied! <pre>print(\"Hello World\")\n</pre> print(\"Hello World\") <pre>Hello World\n</pre>"},{"location":"supervised-learning/classification/k-nearest-neighbors/#k-nearest-neighbors-knn","title":"K-Nearest Neighbors (KNN)\u00b6","text":""},{"location":"supervised-learning/classification/logistic-regression/","title":"Logistic Regression","text":"In\u00a0[1]: Copied! <pre>print(\"Hello World!\")\n</pre> print(\"Hello World!\") <pre>Hello World!\n</pre>"},{"location":"supervised-learning/classification/logistic-regression/#logistic-regression","title":"Logistic Regression\u00b6","text":""},{"location":"supervised-learning/classification/naive-bayes/","title":"Naive Bayes","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn as skl\nimport matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import sklearn as skl import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>from sklearn.datasets import load_iris\n\n# Loading Iris dataset using load_iris\niris_dataset = load_iris()\n\n# Creating dataframe\ndata = pd.DataFrame(data=np.c_[iris_dataset['data'], iris_dataset['target']], columns= iris_dataset['feature_names'] + ['target'])\n\nprint(f\"data is a {type(data).__name__}\")\n</pre> from sklearn.datasets import load_iris  # Loading Iris dataset using load_iris iris_dataset = load_iris()  # Creating dataframe data = pd.DataFrame(data=np.c_[iris_dataset['data'], iris_dataset['target']], columns= iris_dataset['feature_names'] + ['target'])  print(f\"data is a {type(data).__name__}\") <pre>data is a DataFrame\n</pre> <p><code>head()</code>: This function displays the first few rows of the DataFrame, allowing you to get a quick overview of the dataset.</p> In\u00a0[3]: Copied! <pre>data.head()\n</pre> data.head() Out[3]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 <p><code>info()</code>: This method provides a summary of the DataFrame, including the number of rows, column names, data types, and the presence of any missing values.</p> In\u00a0[4]: Copied! <pre>data.info()\n</pre> data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\n 4   target             150 non-null    float64\ndtypes: float64(5)\nmemory usage: 6.0 KB\n</pre> <p><code>value_counts()</code>: Count the occurrences of each species in the 'target' column</p> In\u00a0[5]: Copied! <pre>print(data['target'].value_counts())\n</pre> print(data['target'].value_counts()) <pre>target\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n</pre> <p><code>describe()</code>: By using this, you can generate descriptive statistics of the dataset, such as count, mean, standard deviation, minimum, maximum, and quartile values, for each numerical column in the DataFrame.</p> In\u00a0[6]: Copied! <pre>data.describe()\n</pre> data.describe() Out[6]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target count 150.000000 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 1.000000 std 0.828066 0.435866 1.765298 0.762238 0.819232 min 4.300000 2.000000 1.000000 0.100000 0.000000 25% 5.100000 2.800000 1.600000 0.300000 0.000000 50% 5.800000 3.000000 4.350000 1.300000 1.000000 75% 6.400000 3.300000 5.100000 1.800000 2.000000 max 7.900000 4.400000 6.900000 2.500000 2.000000 <p><code>shape</code>: The shape attribute returns a tuple representing the dimensions of the DataFrame, indicating the number of rows and columns.</p> In\u00a0[7]: Copied! <pre>data.shape\n</pre> data.shape Out[7]: <pre>(150, 5)</pre> <p><code>columns</code>: The columns attribute returns a list of column names in the DataFrame.</p> In\u00a0[8]: Copied! <pre>data.columns\n</pre> data.columns Out[8]: <pre>Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n       'petal width (cm)', 'target'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre>sns.boxplot(data=data)\nplt.xticks(rotation=45)\nplt.title(\"Box Plot of Iris Dataset\")\nplt.show()\n</pre> sns.boxplot(data=data) plt.xticks(rotation=45) plt.title(\"Box Plot of Iris Dataset\") plt.show() <p>The code below removes outliers from the \"sepal width (cm)\" column of the dataset. Outliers are defined as values greater than 3.5 or less than 2.5. By applying a filter, the DataFrame is updated without these outliers, allowing for further analysis or modeling.</p> In\u00a0[10]: Copied! <pre>data = data[\n    (data['sepal width (cm)'] &gt;= 2.5) &amp; (data['sepal width (cm)'] &lt;= 3.5)\n]\nsns.boxplot(data=data)\nplt.xticks(rotation=45)\nplt.title(\"Box Plot of Iris Dataset\")\nplt.show()\n</pre> data = data[     (data['sepal width (cm)'] &gt;= 2.5) &amp; (data['sepal width (cm)'] &lt;= 3.5) ] sns.boxplot(data=data) plt.xticks(rotation=45) plt.title(\"Box Plot of Iris Dataset\") plt.show() In\u00a0[11]: Copied! <pre>sns.pairplot(data, hue=\"target\")\nplt.show()\n</pre> sns.pairplot(data, hue=\"target\") plt.show() <p>The pairplot visualization provides an overview of relationships between different pairs of features in the Iris dataset. By examining the scatterplots, you can identify patterns, correlations, feature importance, and potential outliers. It helps in understanding the distribution of data and making informed decisions regarding feature selection and classification algorithms.</p> In\u00a0[12]: Copied! <pre>correlation_matrix = data.iloc[:, :-1].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Matrix of Iris Dataset\")\nplt.show()\n</pre> correlation_matrix = data.iloc[:, :-1].corr() sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm') plt.title(\"Correlation Matrix of Iris Dataset\") plt.show() <p>The correlation matrix heatmap visually represents the relationships between numeric columns in the Iris dataset. Warm colors indicate positive correlations, cool colors indicate negative correlations, and the intensity represents the strength of the relationship. It provides a quick overview of feature interdependencies and helps identify patterns and potential multicollinearity.</p> In\u00a0[13]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n# Separate the features and target variable\nX = data.drop('target', axis=1)\ny = data['target']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a Naive Bayes classifier\nnb_classifier = GaussianNB()\n\n# Train the classifier on the training data\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the testing data\ny_pred = nb_classifier.predict(X_test)\n\n# Calculate the accuracy of the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n</pre> from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import accuracy_score  # Separate the features and target variable X = data.drop('target', axis=1) y = data['target']  # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Create a Naive Bayes classifier nb_classifier = GaussianNB()  # Train the classifier on the training data nb_classifier.fit(X_train, y_train)  # Make predictions on the testing data y_pred = nb_classifier.predict(X_test)  # Calculate the accuracy of the classifier accuracy = accuracy_score(y_test, y_pred) print(\"Accuracy:\", accuracy) <pre>Accuracy: 0.9166666666666666\n</pre> In\u00a0[14]: Copied! <pre>from sklearn.metrics import confusion_matrix, classification_report\n\n# Print classification report\nprint(\"Classification Report\\n-----------------------\")\nprint(classification_report(y_test, y_pred))\n\n# Plot a confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n</pre> from sklearn.metrics import confusion_matrix, classification_report  # Print classification report print(\"Classification Report\\n-----------------------\") print(classification_report(y_test, y_pred))  # Plot a confusion matrix cm = confusion_matrix(y_test, y_pred) plt.figure(figsize=(8, 6)) sns.heatmap(cm, annot=True, cmap=\"Blues\") plt.xlabel(\"Predicted\") plt.ylabel(\"Actual\") plt.title(\"Confusion Matrix\") plt.show() <pre>Classification Report\n-----------------------\n              precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00         7\n         1.0       0.91      0.91      0.91        11\n         2.0       0.83      0.83      0.83         6\n\n    accuracy                           0.92        24\n   macro avg       0.91      0.91      0.91        24\nweighted avg       0.92      0.92      0.92        24\n\n</pre> <p>The decision chart illustrates the linear separation between the 'petal length (cm)' and 'petal width (cm)' features. It visually displays how the classifier distinguishes between classes based on these two specific features, providing insights into the classification boundaries.</p> In\u00a0[15]: Copied! <pre># Select the first two features for visualization\nX = data[['petal length (cm)', 'petal width (cm)']]\ny = data['target']\n\n# Create a Naive Bayes classifier\nnb_classifier = GaussianNB()\n\n# Fit the Naive Bayes classifier on the selected features\nnb_classifier.fit(X, y)\n\n# Generate a meshgrid to plot the decision boundaries\nx_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\nZ = nb_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Create a scatter plot of the data points with decision boundaries\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap='coolwarm', edgecolors='k')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('Linear Separation of Features')\nplt.show()\n</pre> # Select the first two features for visualization X = data[['petal length (cm)', 'petal width (cm)']] y = data['target']  # Create a Naive Bayes classifier nb_classifier = GaussianNB()  # Fit the Naive Bayes classifier on the selected features nb_classifier.fit(X, y)  # Generate a meshgrid to plot the decision boundaries x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1 y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = nb_classifier.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)  # Create a scatter plot of the data points with decision boundaries plt.figure(figsize=(8, 6)) plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm') plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap='coolwarm', edgecolors='k') plt.xlabel('Sepal Length (cm)') plt.ylabel('Sepal Width (cm)') plt.title('Linear Separation of Features') plt.show() <pre>/src/ml-essentials/venv/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"supervised-learning/classification/naive-bayes/#naive-bayes","title":"Naive Bayes\u00b6","text":""},{"location":"supervised-learning/classification/naive-bayes/#problem-statement","title":"Problem Statement\u00b6","text":"<p>The Iris dataset is a widely-used dataset in machine learning. It contains measurements of 150 Iris flowers from three species: <code>Setosa</code>, <code>Versicolor</code>, and <code>Virginica</code>. The dataset includes four numerical features (shown in figure):</p> <ul> <li><code>sepal length</code>:  length in cm</li> <li><code>sepal width</code>: width in cm</li> <li><code>petal length</code>: length in cm</li> <li><code>petal width</code>: width in cm</li> <li><code>target</code>: species index [<code>0</code>, <code>1</code>, <code>2</code>] which repectivly indicates [<code>Setosa</code>, <code>Versicolor</code>, <code>Virginica</code>]</li> </ul> <p>The goal is to classify the flowers correctly based on these features. It is a balanced dataset with equal samples for each species, making it a popular choice for classification algorithms like Naive Bayes.</p>"},{"location":"supervised-learning/classification/naive-bayes/#import-necessary-packages","title":"Import necessary packages\u00b6","text":"<p>We will be using the following packages for this task:</p> <ul> <li><code>pandas</code> to load and manipulate data</li> <li><code>numpy</code> to work with arrays of data</li> <li><code>matplotlib</code> or <code>seaborn</code> for plotting and visualization</li> <li><code>sklearn</code> for machine learning and data analysis tools</li> </ul>"},{"location":"supervised-learning/classification/naive-bayes/#load-the-data-using-sklearn","title":"Load the data using sklearn\u00b6","text":"<p>The Iris dataset can be downloaded from the UCI Machine Learning Repository at the following link: Iris Dataset. But instead of directly downloading the Iris dataset, we can conveniently access it using the <code>sklearn.datasets</code> library.</p> <p>In our excercise we will use <code>load_iris()</code> function from the <code>sklearn.datasets</code> module to load the dataset directly and using <code>panda</code> we create dataframe.</p>"},{"location":"supervised-learning/classification/naive-bayes/#exploring-data-information-with-pandas","title":"Exploring Data Information with Pandas\u00b6","text":"<p>Once we have loaded the Iris dataset into a pandas DataFrame, you can easily explore the data's information using various pandas functions. Here are some common pandas methods you can utilize to gain insights into the dataset:</p>"},{"location":"supervised-learning/classification/naive-bayes/#visualize-the-data","title":"Visualize the data\u00b6","text":"<p>In this section we plot will boxplot and pairplot that are used to understand the distribution, outliers, patterns, and correlations in the Iris dataset.</p>"},{"location":"supervised-learning/classification/naive-bayes/#correlation-matrix","title":"Correlation matrix\u00b6","text":"<p>To get a numerical idea about the correlation between the features and the target variable, we will draw a correlation matrix.</p>"},{"location":"supervised-learning/classification/naive-bayes/#naive-bayes-classification","title":"Naive Bayes Classification\u00b6","text":"<p>Naive Bayes classification is a machine learning algorithm that uses Bayes' theorem to predict the probability of different classes based on input features. It assumes that features are conditionally independent. It is simple, efficient, and effective for high-dimensional data, making it widely used in various domains such as text classification and spam filtering. Lecture notes on Bayes' theorem (University of Washington).</p>"},{"location":"supervised-learning/classification/naive-bayes/#classification-report","title":"Classification Report\u00b6","text":"<p>In this section, the Naive Bayes classifier's performance is assessed using a confusion matrix and a classification report. Additionally, a decision chart is plotted to visualize the linear separation between two features, providing insights into the classifier's performance and its ability to distinguish between classes.</p>"},{"location":"supervised-learning/regression/","title":"Introduction to Linear Regression","text":"<p>Author: Mir Sazzat Hossain</p>"},{"location":"supervised-learning/regression/#what-is-regression","title":"What is Regression?","text":"<p>Linear regression is a supervised learning algorithm that is used to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable). The variable we are using to predict the other variable's value is called the independent variable (or sometimes, the predictor variable).</p> <p>For example, you could use linear regression to predict the price of a book based on its number of pages. In this case, the price of the book is the dependent variable and the number of pages is the independent variable.</p>"},{"location":"supervised-learning/regression/#how-does-linear-regression-work","title":"How Does Linear Regression Work?","text":"<p>Linear regression works by finding the best fit line for the data points. The best fit line is the line that minimizes the sum of squared errors (SSE) between the predicted values and the actual values. The predicted values are the values that are predicted by the linear function. The actual values are the values that are actually observed in the data.</p> <p>The equation for the best fit line is:</p> \\[y = mx + b\\] <p>where \\(y\\) is the predicted value, \\(m\\) is the slope of the line, \\(x\\) is the independent variable, and \\(b\\) is the y-intercept.</p> <p>The slope of the line is the change in \\(y\\) divided by the change in \\(x\\). The y-intercept is the value of \\(y\\) when \\(x\\) is equal to zero.</p>"},{"location":"supervised-learning/regression/#example-predicting-the-price-of-a-book-based-on-its-number-of-pages","title":"Example: Predicting the Price of a Book Based on Its Number of Pages","text":"<p>Let's look at the example we used earlier. We want to predict the price of a book based on its number of pages. We have the following data:</p> Number of Pages Price 166 54 195 82 200 72 260 72 265 90 335 124 370 94 450 118 517 152 552 132 <p>If we plot this data on a graph, we can see that there is a positive correlation between the number of pages and the price. This means that as the number of pages increases, the price also increases.</p> <p></p> <p>We can draw a line through the data points that best fits the data i.e., the line that minimizes the sum of squared errors (SSE) between the predicted values and the actual values. This line is called the best fit line.</p> <p></p> <p>The green dotted lines are the errors between the predicted values and the actual values. And the red line is the best fit line, which minimizes the sum of squared errors (SSE) between the predicted values and the actual values. The equation for the best fit line for this data is:</p> \\[y = 0.2x + 31.22\\] <p>here \\(0.2\\) is the slope of the line and \\(31.22\\) is the y-intercept. How did we get these values?</p>"},{"location":"supervised-learning/regression/#how-to-find-the-best-fit-line","title":"How to Find the Best Fit Line?","text":"<p>As previously mentioned, the best fit line is the line that minimizes the sum of squared errors (SSE) between the predicted values and the actual values. Now, should we just try out different lines and see which one minimizes the SSE? No, that would be impossible. There are an infinite number of lines that we could try out. So, how do we find the best fit line?</p> <p>The SSE formula looks like this:</p> \\[SSE = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\\] <p>where \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.</p> <p>We want to minimize the SSE. To do that, we can take the following steps:</p> <ul> <li>Take the derivative of the SSE formula with respect to the slope \\(m\\)</li> <li>Set the derivative equal to zero</li> <li>Solve for \\(m\\)</li> <li>Take the derivative of the SSE formula with respect to the y-intercept \\(b\\)</li> <li>Set the derivative equal to zero</li> <li>Solve for \\(b\\)</li> </ul> <p>To much math? Don't worry, won't go into the details of the math. We'll just give you the final formula for the best fit line:</p> \\[m = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\] \\[b = \\bar{y} - m\\bar{x}\\] <p>where \\(\\bar{x}\\) is the mean of the independent variable and \\(\\bar{y}\\) is the mean of the dependent variable.</p> <p>This is how we got the values for the slope and the y-intercept in the previous example.</p>"},{"location":"supervised-learning/regression/#how-to-use-linear-regression-to-predict-the-price-of-a-book","title":"How to Use Linear Regression to Predict the Price of a Book?","text":"<p>Now that we have the best fit line, we can use it to predict the price of a book based on its number of pages. Let's say we have a book with 300 pages. We can use the best fit line to predict the price of the book:</p> \\[ \\begin{align} y &amp;= mx + b \\\\ &amp;= 0.2(300) + 31.22 \\\\ &amp;= 91.22 \\end{align} \\] <p>So, the predicted price of the book is \\(91.22\\).</p>"},{"location":"supervised-learning/regression/#conclusion","title":"Conclusion","text":"<p>That's it! You now know how linear regression works. You can use it to predict the value of a variable based on the value of another variable. Even though we didn't go into the details of the math, we hope you now have a better understanding of how linear regression works. In the next article, we'll look at how to implement linear regression in Python. See you there!</p>"},{"location":"supervised-learning/regression/linear-regression/","title":"Simple Linear Regression","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns In\u00a0[2]: Copied! <pre>data = pd.read_csv('datasets/Advertising.csv')\ndata.head()\n</pre> data = pd.read_csv('datasets/Advertising.csv') data.head() Out[2]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 12.0 3 151.5 41.3 58.5 16.5 4 180.8 10.8 58.4 17.9 In\u00a0[3]: Copied! <pre>data.info()\n</pre> data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   TV         200 non-null    float64\n 1   Radio      200 non-null    float64\n 2   Newspaper  200 non-null    float64\n 3   Sales      200 non-null    float64\ndtypes: float64(4)\nmemory usage: 6.4 KB\n</pre> <p>From the output, we can see that the dataframe has 200 rows and 4 columns. The columns are named <code>TV</code>, <code>Radio</code>, <code>Newspaper</code>, and <code>Sales</code>. The <code>Sales</code> column is the target variable and the other three columns are the features. The <code>info()</code> function also shows that there are no missing values (null values) in the dataframe and all the values are numerical (float64).</p> <p>Next, we will use the <code>describe()</code> function to get some statistical information about the dataframe.</p> In\u00a0[4]: Copied! <pre>data.describe()\n</pre> data.describe() Out[4]: TV Radio Newspaper Sales count 200.000000 200.000000 200.000000 200.000000 mean 147.042500 23.264000 30.554000 15.130500 std 85.854236 14.846809 21.778621 5.283892 min 0.700000 0.000000 0.300000 1.600000 25% 74.375000 9.975000 12.750000 11.000000 50% 149.750000 22.900000 25.750000 16.000000 75% 218.825000 36.525000 45.100000 19.050000 max 296.400000 49.600000 114.000000 27.000000 <p>From the output, we can see that the average sales revenue is 15.13 thousand dollars with a standard deviation of 5.29 thousand dollars. The minimum sales revenue is 1.6 thousand dollars and the maximum sales revenue is 27 thousand dollars. The average advertising budget for TV, Radio, and Newspaper are 147, 23, and 30 thousand dollars respectively.</p> <p>Also, we got a brief idea about the distribution of the data from the <code>describe()</code> function. For example, the maximum advertising budget for TV is 296 thousand dollars and the minimum is 0.7 thousand dollars. The average advertising budget for TV is 147 thousand dollars and the standard deviation is 85 thousand dollars. The 25th percentile, 50th percentile, and 75th percentile values are 74, 149, and 218 thousand dollars respectively meaning that 25% of the markets have an advertising budget of less than 74 thousand dollars, 50% of the markets have an advertising budget of less than 149 thousand dollars, and 75% of the markets have an advertising budget of less than 218 thousand dollars for TV. The same information can be obtained for the other features.</p> In\u00a0[5]: Copied! <pre>sns.boxplot(data=data, orient='v')\nplt.show()\n</pre> sns.boxplot(data=data, orient='v') plt.show() <p>From the boxplots, we can see that there are two outliers in the <code>Newspaper</code> feature. The outliers are the two points above the maximum value. We will remove these two points from the dataset.</p> In\u00a0[6]: Copied! <pre>data = data[data['Newspaper'] &lt; 90]\nsns.boxplot(data=data, orient='v')\nplt.show()\n</pre> data = data[data['Newspaper'] &lt; 90] sns.boxplot(data=data, orient='v') plt.show() <p>From the boxplots, we can see that there are no outliers in the <code>Newspaper</code> feature.</p> In\u00a0[7]: Copied! <pre>sns.pairplot(data=data, diag_kind=\"kde\")\nplt.show()\n</pre> sns.pairplot(data=data, diag_kind=\"kde\") plt.show() <p>From the pairplot, we can conclude the following:</p> <ul> <li>No or very little linear relationship between TV and Radio</li> <li>Low linear relationship between TV and Newspaper</li> <li>Moderate linear relationship between Radio and Newspaper</li> <li>High linear relationship between TV and Sales, Radio and Sales, and Newspaper and Sales</li> <li>A small curvilnear relationship between TV and Sales as well as Radio and Sales</li> </ul> In\u00a0[19]: Copied! <pre>matrix = data.corr()\nsns.heatmap(matrix, annot=True, cmap='Blues')\nplt.show()\n</pre> matrix = data.corr() sns.heatmap(matrix, annot=True, cmap='Blues') plt.show() <p>This plot further quantifies the relationship between the features and the target variable, and confirms our observations from the pairplot.</p>"},{"location":"supervised-learning/regression/linear-regression/#simple-linear-regression","title":"Simple Linear Regression\u00b6","text":""},{"location":"supervised-learning/regression/linear-regression/#problem-statement","title":"Problem Statement\u00b6","text":"<p>In this notebook, we will predict sales revenue for a given advertising budget using a simple linear regression model. For this purpose, we will use the kaggle dataset Advertising.csv. The dataset consists of 200 rows of data, each representing a market with the following attributes:</p> <ul> <li><code>TV</code>: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)</li> <li><code>Radio</code>: advertising dollars spent on Radio</li> <li><code>Newspaper</code>: advertising dollars spent on Newspaper</li> <li><code>Sales</code>: sales of a single product in a given market (in thousands of widgets)</li> </ul> <p>We will try to predict sales using the three other features.</p>"},{"location":"supervised-learning/regression/linear-regression/#import-necessary-packages","title":"Import necessary packages\u00b6","text":"<p>We will use the following packages for this tutorial:</p> <ul> <li><code>pandas</code> to load and manipulate data</li> <li><code>numpy</code> to work with arrays of data</li> <li><code>matplotlib</code> and <code>seaborn</code> for plotting and visualization</li> <li><code>sklearn</code> for machine learning and data analysis tools</li> </ul>"},{"location":"supervised-learning/regression/linear-regression/#load-the-data","title":"Load the data\u00b6","text":"<p>Download the dataset from here. Click on the folder icon on the left panel of colab and create a folder named <code>datasets</code>. Then upload the dataset to the <code>datasets</code> folder.</p> <p>We will use the <code>pd.read_csv()</code> function to read the dataset into a pandas dataframe. We will also use the <code>head()</code> function to display the first five rows of the dataframe.</p>"},{"location":"supervised-learning/regression/linear-regression/#explor-the-data","title":"Explor the data\u00b6","text":"<p>At first, we will try to understand the data by exploring it. We will use the <code>info()</code> function to get a brief information about the dataframe.</p>"},{"location":"supervised-learning/regression/linear-regression/#visualize-the-data","title":"Visualize the data\u00b6","text":"<p>To get a better understanding of the data, we will visualize it using the <code>matplotlib</code> and <code>seaborn</code> packages.</p>"},{"location":"supervised-learning/regression/linear-regression/#boxplot","title":"Boxplot\u00b6","text":"<p>At first we will draw a boxplot for each of the features to get an idea about the distribution of the data and to identify any outliers.</p>"},{"location":"supervised-learning/regression/linear-regression/#remove-outliers","title":"Remove outliers\u00b6","text":"<p>We will use update the dataframe by taking only the rows where the budget for <code>Newspaper</code> is less than 90 thousand dollars, as the outliers are above 90 thousand dollars.</p>"},{"location":"supervised-learning/regression/linear-regression/#pairplot","title":"Pairplot\u00b6","text":"<p>Next, we will draw a pairplot to visualize the relationship between the features and the target variable. The pairplot will also show the correlation between the features and the distribution of the data.</p>"},{"location":"supervised-learning/regression/linear-regression/#correlation-matrix","title":"Correlation matrix\u00b6","text":"<p>To get a numerical idea about the correlation between the features and the target variable, we will draw a correlation matrix.</p>"},{"location":"supervised-learning/regression/linear-regression/#prepare-the-data","title":"Prepare the data\u00b6","text":"<p>Before we can train a machine learning model, we need to prepare the data.</p>"},{"location":"supervised-learning/tree-based/","title":"Introduction to Tree Based Algorithms","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur. Donec ut libero sed arcu vehicula ultricies a non tortor. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut gravida lorem. Ut turpis felis, pulvinar a semper sed, adipiscing id dolor. Pellentesque auctor nisi id magna consequat sagittis. Curabitur dapibus enim sit amet elit pharetra tincidunt feugiat nisl imperdiet. Ut convallis libero in urna ultrices accumsan. Donec sed odio eros. Donec viverra mi quis quam pulvinar at malesuada arcu rhoncus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In rutrum accumsan ultricies. Mauris vitae nisi at sem facilisis semper ac in est.</p>"},{"location":"supervised-learning/tree-based/decision-tree/","title":"Decision Tree","text":"In\u00a0[1]: Copied! <pre>print(\"Hello World!\")\n</pre> print(\"Hello World!\") <pre>Hello World!\n</pre>"},{"location":"supervised-learning/tree-based/decision-tree/#decision-tree","title":"Decision Tree\u00b6","text":""}]}